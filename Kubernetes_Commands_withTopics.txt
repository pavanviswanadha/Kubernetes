Imp links : https://killercoda.com/cka
                   https://killer.sh/
				   
				  
				  
==============================
SERVICE MESH
==============================

Istio is a service mesh whcih is used to manage the network traffic between the different services, by creating a side car container using MTLS.

To istall kiali 

kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.21/samples/addons/kiali.yaml

To view istio kiali dasboard  - to monitor application in all namespaces of k8cluster

istioctl dashboard kiali


====================== Basic commands =================================

kubectl get pods

Kubectl get All

kubectl describe pods newpods-9f9dn - to get details and images used to create a pod
Node:             controlplane/192.48.108.6 - which node pods are placed 

kubectl exec -it <sample-pod-name> /bin/bash ----- To login into a pod and check application status 

kubectl run nginx --image=nginx  ------ Normal pod creation 

kubectl apply -f pod.yml ---- creating pod with pod.yml created a pod

kubectl get pod webapp -o yaml > my-new-pod.yaml - to export pod definition to a yaml

================ Replicat sets ===================

kubectl create -f rc-definition.yml  ------ create replicat set with yml file 

kubectl get replicationcontroller      
kubectl get replicasets
kubectl describe replicasets
kubectl delete replicasets <replicaset-name>
kubectl edit replicasets <new-replica-set>  -------- if we need to delete old pods so new replicas will create automatically

kubectl replace -f replicaset-definition.yml ----------- TO replace definition file of replicaset
kubectl scale --replicas=6 -f replicaset-definition.yml  --------------- To apply replicaset in replicaset-definition.yml
kubectl scale --replicas=6 replicasets <relicaset_myapp_name> ------------ To scale replicasets 

========== Cluster info commands ===============

kubectl cluster-info  -------- To get cluster into
kubectl get pods --namespace=kube-system  -------- To view all componentes in master node in kube-system namespace
DSGUY5NBNJNUHU
kubectl get componentstatuses ----- to getAPI and controller status
kubectl logs -n kube-system <api-server-pod-name> kube-apiserver  ---- to check logs of api nod

=============Deployments ====================

kubectl get deployments
kubectl create -f deployment-definition-1.yaml
---- IN deployment yml kind should be with "Deployment "
apiVersion: apps/v1
kind: Deployment

Kubectl create deployment <deployment name> --image=<image name> --replicas=3

================Services=======================
Kubectl get services or kubectl get svc 
Kubectl describe svc <service_name>

Types of services :
clusterIp
Nodeport
Load balancing 

Why services are used :
Loadbalancing
Service discovery
Exposing application

==============Namespaces ========================

Mysql.connect("db-serivce.dev.svc.cluster.local")             <servicenamd.namespace.servicename.domain>

kubectl run redis --image=redis -n finance    to create a pod in a particular name space with name and image name only 

Kubectl get pods --namespcae=kube-system  --------get pods in a particular namespace

Kubectl get create -f <pod-definiton.yml> --namespace=<dev>   --------to create pod to particular name space 

Kubectl config set-context $(kubectl config current-context) --namespace=dev    ---------to move current names pods  to desired namespace example "dev"

kubectl get namespaces ---- to get all namespaces in k8's
 Kubectl get pods --all-namespaces   --------to get pods in all namespaces 

ResourceQuota - is used to limit the resouces to a particular name space 

Kubectl create -f compute-quota.yml   ----- to create resoursese like cpu, ram for a particular namespace 

==================== Imperative commands =============================

Detailed step to step process what to do 

Create objects
Kubectl run --image=nginx nginx
Kubectl creat deployment --image=nginx nginx 
Kubectl expose deployment nginx --port 80 
Update objects
Kubectl edit deployment nginx
Kubectl scale deployment nginx --relicas=5
Kubectl set image deployment nginx nginx=nginx:1.18
Kubectl create -f nginx.yaml
Kubectl replace -f nginx.yaml
Kubectl delete -f nginx.yaml

======================Declative commands==============================

It is used to make it declarative and applied  changes to the object are in record

Create objects 
Kubectl apply -f nginx.yaml
Kubectl apply -f /path/to/config-files  ----- it will apply change to objects in side the path

Update objects 
Kubectl apply of nginx.yml

===================================================================



kubectl run redis  --image=redis:alpine --dry-run=client -o yaml > redis-pod.yaml

kubectl create -f redis-pod.yaml

POD
Create an NGINX Pod
kubectl run nginx --image=nginx

Generate POD Manifest YAML file (-o yaml). Don't create it(--dry-run)
kubectl run nginx --image=nginx --dry-run=client -o yaml

Deployment
Create a deployment
kubectl create deployment --image=nginx nginx

Generate Deployment YAML file (-o yaml). Don't create it(--dry-run)
kubectl create deployment --image=nginx nginx --dry-run=client -o yaml

Generate Deployment with 4 Replicas
kubectl create deployment nginx --image=nginx --replicas=4

You can also scale a deployment using the kubectl scale command.
kubectl scale deployment nginx --replicas=4
Another way to do this is to save the YAML definition to a file and modify
kubectl create deployment nginx --image=nginx --dry-run=client -o yaml > nginx-deployment.yaml

You can then update the YAML file with the replicas or any other field before creating the deployment.

Service
Create a Service named redis-service of type ClusterIP to expose pod redis on port 6379
kubectl expose pod redis --port=6379 --name redis-service --dry-run=client -o yaml
(This will automatically use the pod's labels as selectors)
Or
kubectl create service clusterip redis --tcp=6379:6379 --dry-run=client -o yaml (This will not use the pods labels as selectors, instead it will assume selectors as app=redis. You cannot pass in selectors as an option. So it does not work very well if your pod has a different label set. So generate the file and modify the selectors before creating the service)

Create a Service named nginx of type NodePort to expose pod nginx's port 80 on port 30080 on the nodes:
kubectl expose pod nginx --type=NodePort --port=80 --name=nginx-service --dry-run=client -o yaml
(This will automatically use the pod's labels as selectors, but you cannot specify the node port. You have to generate a definition file and then add the node port in manually before creating the service with the pod.)
Or
kubectl create service nodeport nginx --tcp=80:80 --node-port=30080 --dry-run=client -o yaml
(This will not use the pods labels as selectors)
Both the above commands have their own challenges. While one of it cannot accept a selector the other cannot accept a node port. I would recommend going with the kubectl expose command. If you need to specify a node port, generate a definition file using the same command and manually input the nodeport before creating the service.

==================================================

==========labesl and Selectors========================

kubectl get namespace default --show-labels

kubectl get pods --selector env=dev

kubectl get pods --selector env=dev --no-headers| wc -l 

kubectl get pods --selector bu=finance

kubectl get all --selector env=prod   ---------- to get all detail of particular environment 

kubectl get all --selector env=prod --no-headers | wc -l

kubectl get all --selector env=prod,BU=finance,tier=frontend

kubectl label node node01 color=blue ------------to add a new label to a particular node 

kubectl create deployment blue --replicas=3 --image=nginx -------- to create a deployment with image nginx and replicas 3 


==========Taints and Tolerants ==========================

Taints are for nodes  - NoSchedule | PreferNoSchedule | NoExecute 
Tolerants are for pods 
when we are applying tolerants in YAML it should be in "" double quotes.

Taints and tolerations doesnot tell a pod to go for a particular node. 

Why we will not schedule pod on master there is default taint set on master NoSchedule. To view it run below command.
kubectl describe node kubemaster | grep Taint 
kubectl describe node node01 | grep -i taints ------- to get taints on a node 
 
 
kubectl taint nodes <node-name> key=value:NoSchedule ------ to maintain taint effect on a node 
kubectl taint nodes node1 app=blue:NoSchedule 
NoSchedule - which means the pods will not be schedule on the node 
PreferNoSchedule - which means system will try to not place a pod on the node 
NoExecute  - which means new pods will not be scheduled and the if any pods will be avicted if the are not tolerate the taint. 

kubectl get nodes
kubectl describe node node01 | grep -i taint
kubectl taint nodes node01 spray=mortein:NoSchedule
kubectl run mosquito  --image=nginx ------ To create a pod with name mosquito and image=nginx
kubectl run bee --image=nginx --restart=Never  -------It will create a pod a standalone 
kubectl patch pod bee -p '{"spec": {"tolerations": [{"key": "spray","value": "mortein", "operator": "Equal", "effect": "NoSchedule"}]}}' -----This command patches the bee pod to add a toleration for the taint with the key
kubectl taint nodes controlplane node-role.kubernetes.io/control-plane:NoSchedule- --------------- To remove taints from a particular node 


======================= Node selectors ===================
Node selectors are use to define pod to create in a particular node utilize the resources of the node with large mermory.
(it is limited to a particular node and to over come this we have node affinity)

To use node selectors first we need to have the nodes labeled.
kubectl label nodes <node-name> <label-key>=<label-value>
kubectl label nodes node1 size=Large

we need to add nodeSelectors: under spec: with node <label-key>=<label-value>

We need to right "or" and "not" operation with node selector so we have node affinity to fix it.

=====================Node Affinity ==================
Node Affinity is used to create pods on desired nodes with lables depends upon requirements.

how come if the label was changed in midle for some reasons so they is node affinity type 
requiredDuringSchedulingIgnoredDuringExecution 
preferedDuringSchedulingIgnoredDuringExecution


====================Daemon Sets=====================
kubectl get  daemonsets --all-namespaces
kubectl create deployment elasticsearch --image=registry.k8s.io/fluentd-elasticsearch:1.20 -n kube-system --dry-run=client -o yaml > fluentd.yaml
kubectl create -f fluentd.yaml
kubectl apply -f fluentd.yaml
kubectl delete daemonsets elasticsearch -n kube-system

===============Static Pods ========================

kubectl run --restart=Never --image=busybox static-busybox --dry-run=client -o yaml --command -- sleep 1000 > /etc/kubernetes/manifests/static-busybox.yaml ------- to create a static pod
kubectl get pods --all-namespaces 
/etc/kubernetes/manifests/ - in this path we need create pod definitions to create static pods
kubectl run --restart=Never --image=busybox:1.28.4 static-busybox --dry-run=client -o yaml --command -- sleep 1000 > /etc/kubernetes/manifests/static-busybox.yaml  ------ to update existing image in static pod

TO delete a static pod created we need to follow below steps 
kubectl get nodes -o wide  --- get the node details and SSH to the node 
var/lib/kubelet/config.yaml ----- get the static pod manifest file path 
staticPodPath: /etc/just-to-mess-with-you ---- go to this path and delete the pod definiton file

===================Customize kub-schedulers ==========================
kubectl get pods -n=kube-system
kubectl describe pod kube-scheduler-controlplane -n=kube-system
kubectl get serviceaccount -n kube-system ---- to get services 
kubectl get clusterrolebinding  

to create a custome scheduler we can use in config file "kubectl create -f /root/my-scheduler-configmap.yaml"
apiVersion: v1
data:
  my-scheduler-config.yaml: |
    apiVersion: kubescheduler.config.k8s.io/v1
    kind: KubeSchedulerConfiguration
    profiles:
      - schedulerName: my-scheduler
    leaderElection:
      leaderElect: false
kind: ConfigMap
metadata:
  creationTimestamp: null
  name: my-scheduler-config
  namespace: kube-system
  
================ scheduler profiles ================
We can make pod scheduled in priority by using priority class which definied separately 

Below are the shceduling plugins used to schedule a pod depends on requirement.
Scheduling queue   ---->    Filtering      ---->    Scoring        ---->    Binding
 PrioritySort     ---->  NodeResourcesFit  ---->  NodeResoursesFit ----> DefaulBinder
                           NodeName               ImageLocality
						  NodeUnschedulable 
						  
For all scheduling plugins we have some extension points which are used for required purposes like prefilter-filter-postFilter , preScore-Score-postScore, preBind-Bind-PostBind

==============================================================
Logging and Monitoring 
==============================================================
clone -  git clone https://github.com/kodekloudhub/kubernetes-metrics-server.git
create metrics deployment ------- kubectl create -f .
kubectl top node - to get utilization of node cpu and memory 
kubectl top pod  - to get memory utilized by a pod
kubectl top pod --all-namespaces











